{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of Methods of Storing Data in `numpy` Arrays on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "import tables\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction import (FeatureHasher,\n",
    "                                        DictVectorizer)\n",
    "\n",
    "from src import parse_non_nlp_features_string\n",
    "from src.mongodb import connect_to_db\n",
    "from src.experiments import (make_cursor,\n",
    "                             get_data_point,\n",
    "                             ExperimentalData)\n",
    "from src.datasets import get_bin_ranges_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to reviews database\n",
    "db = connect_to_db(host='localhost', port=37017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n"
     ]
    }
   ],
   "source": [
    "# Get some data for \"Dota_2\" with the label \"total_game_hours\" and with\n",
    "# the number of bins set to 3 and the bin factor set to 6.0\n",
    "game = 'Dota_2'\n",
    "label = 'total_game_hours'\n",
    "nbins = 3\n",
    "bin_factor = 6.0\n",
    "#bin_ranges = get_bin_ranges_helper(db, [game], label, nbins, bin_factor)\n",
    "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n",
    "print(\"bin_ranges = {}\".format(bin_ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = ExperimentalData(db=db, prediction_label=label, games=[game],\n",
    "                        folds=1, fold_size=30, grid_search_folds=0,\n",
    "                        grid_search_fold_size=0, bin_ranges=bin_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['5690a60fe76db81bef5c4613', '5690a60fe76db81bef5c30ed',\n",
       "        '5690a60fe76db81bef5c322d', '5690a60fe76db81bef5c49ce',\n",
       "        '5690a60fe76db81bef5c28ce', '5690a60fe76db81bef5c44b0',\n",
       "        '5690a60fe76db81bef5c4987', '5690a60fe76db81bef5c496c',\n",
       "        '5690a60fe76db81bef5c3e3f', '5690a60fe76db81bef5c4b13',\n",
       "        '5690a60fe76db81bef5c403e', '5690a60fe76db81bef5c3cec',\n",
       "        '5690a60fe76db81bef5c2e06', '5690a60fe76db81bef5c4464',\n",
       "        '5690a60fe76db81bef5c3fd2', '5690a60fe76db81bef5c3510',\n",
       "        '5690a60fe76db81bef5c36bb', '5690a60fe76db81bef5c2f74',\n",
       "        '5690a60fe76db81bef5c2f51', '5690a60fe76db81bef5c4479',\n",
       "        '5690a60fe76db81bef5c4ad5', '5690a60fe76db81bef5c47ff',\n",
       "        '5690a60fe76db81bef5c3f2a', '5690a60fe76db81bef5c3872',\n",
       "        '5690a60fe76db81bef5c2a3f', '5690a60fe76db81bef5c4465',\n",
       "        '5690a60fe76db81bef5c2750', '5690a60fe76db81bef5c46c6',\n",
       "        '5690a60fe76db81bef5c2aba', '5690a60fe76db81bef5c3973'], \n",
       "       dtype='<U24')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have a set of IDs to work with, which point to samples in the\n",
    "# dataset\n",
    "data.training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5690a60fe76db81bef5c4613',\n",
       " '5690a60fe76db81bef5c30ed',\n",
       " '5690a60fe76db81bef5c322d',\n",
       " '5690a60fe76db81bef5c49ce',\n",
       " '5690a60fe76db81bef5c28ce',\n",
       " '5690a60fe76db81bef5c44b0',\n",
       " '5690a60fe76db81bef5c4987',\n",
       " '5690a60fe76db81bef5c496c',\n",
       " '5690a60fe76db81bef5c3e3f',\n",
       " '5690a60fe76db81bef5c4b13']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = list(data.training_set[0])\n",
    "data_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_achievements_possible', 'num_comments', 'num_voted_helpfulness', 'num_achievements_attained', 'num_groups', 'num_badges', 'friend_player_level', 'num_workshop_items', 'found_helpful_percentage', 'num_found_unhelpful', 'num_achievements_percentage', 'num_found_helpful', 'num_screenshots', 'num_friends', 'num_games_owned', 'num_guides', 'num_reviews', 'num_found_funny'}\n"
     ]
    }
   ],
   "source": [
    "# For our features, we will use all of the NLP features + the review/reviewer\n",
    "# attributes that are not directly related to the label (\"total_game_hours\")\n",
    "non_nlp_feature_set_labels = parse_non_nlp_features_string(\"all\", label)\n",
    "print(non_nlp_feature_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll get the actual labels and the corresponding features for each\n",
    "# sample\n",
    "y = []\n",
    "X = []\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    y.append(sample.get('y'))\n",
    "    X.append(sample.get('x'))\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 1, 2, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example labels\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of rat', 1),\n",
       " ('crust :', 1),\n",
       " (\"-visage 's\", 1),\n",
       " ('always hate', 1),\n",
       " ('oasted', 1),\n",
       " ('pirotity', 1),\n",
       " ('tea', 1),\n",
       " ('their saying', 1),\n",
       " ('world can', 1),\n",
       " ('review that', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example features (sparse format)\n",
    "list(X[0].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll make a vectorizer object (actually, two vectors, one with `DictVectorizer`\n",
    "# and the other with `FeatureHasher`) and fit with `X`\n",
    "dict_vec = DictVectorizer(sparse=True)\n",
    "feature_hasher_vec = FeatureHasher(n_features=2**18, non_negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vec.fit(X)\n",
    "feature_hasher_vec.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_hasher_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we must transform `X` with the vectorizers to get the sparse scipy arrays\n",
    "X_dict_vectorized = dict_vec.transform(X)\n",
    "X_feature_hasher_vectorized = feature_hasher_vec.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30x150447 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4223615 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_dict_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_feature_hasher_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,  140430,  280869,  421947,  562705,  706627,  847056,\n",
       "        987834, 1128189, 1268661, 1409102, 1549611, 1689989, 1830817,\n",
       "       1971240, 2111773, 2252133, 2392525, 2532868, 2673419, 2814388,\n",
       "       2954836, 3096452, 3237277, 3378794, 3521468, 3662042, 3802450,\n",
       "       3942804, 4083194, 4223615], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,  108600,  217212,  326169,  434956,  545560,  654160,\n",
       "        762970,  871527,  980153, 1088762, 1197416, 1305980, 1414819,\n",
       "       1523405, 1632054, 1740618, 1849192, 1957736, 2066414, 2175328,\n",
       "       2283926, 2393221, 2502043, 2611281, 2721157, 2829832, 2938418,\n",
       "       3046972, 3155548, 3264138], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_vectorized.indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `pytables` to Store `scipy` Arrays to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scipy` sparse arrays unfortunately cannot be stored with `pytables`; however, they can be converted to dense arrays and then stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open new empty HDF5 files\n",
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\", mode=\"w\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the root groups\n",
    "root_dict_vectorized = X_dict_vectorized_dense_file.root\n",
    "root_feature_hasher_vectorized = X_feature_hasher_dense_file.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the dense arrays on the HDF5 files\n",
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.create_array(root_dict_vectorized,\n",
    "                                              'X_dict_vectorized_dense',\n",
    "                                              X_dict_vectorized.todense(),\n",
    "                                              \"X dict vectorized dense\")\n",
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.create_array(root_feature_hasher_vectorized,\n",
    "                                             'X_feature_hasher_dense',\n",
    "                                             X_feature_hasher_vectorized.todense(),\n",
    "                                             \"X feature hasher vectorized dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mmulholland mmulholland 35M Mar  8 00:16 X_dict_vectorized_dense.h5\r\n",
      "-rw-rw-r-- 1 mmulholland mmulholland 61M Mar  8 00:16 X_feature_hasher_dense.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh X*_dense.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, to store even a relatively small 30-sample dataset vectorized with `DictVectorizer` in a dense format, it can be 35 MB\n",
    "- Suprisingly, storing the same dataset vectorized with `FeatureHasher`, which is supposed to be memory-efficient, requires even more memory to store the same data (61 MB) in a dense format\n",
    "- Let's see what we can do with the array that's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_dict_vectorized_dense (Array(30, 150447)) 'X dict vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_feature_hasher_dense (Array(30, 262144)) 'X feature hasher vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can a model be trained with `pytables` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc1 = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.predict(X_dict_vectorized_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2 = Perceptron()\n",
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2.predict(X_feature_hasher_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, it seems that `pytables` data can be used as a drop-in replacement for non-sparse `numpy` arrays (or `todense`-converted sparse `scipy` arrays as generated via `DictVectorizer`/`FeatureHasher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to use `pytables` in order to increase memory efficiency, a possible algorithm would be the following:\n",
    "    - Extract and vectorize data\n",
    "    - Use `todense` to make the arrays dense\n",
    "    - Save to data an `hdf5` file with `pytables`\n",
    "    - Remove the original data so that it gets garbage-collected\n",
    "    - Use `pytables` arrays in place of data wherever needed\n",
    "    - Remove the `hdf5` files after complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that it is possible to create enlargeable arrays with `pytables`, so it's possible that an array file could be generated, saved, and closed, and then reopened and enlargened and stored again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the `hdf5` files have been created, arrays saved to them, and then closed, let's try to read in the data again and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.root.X_dict_vectorized_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_dict_vectorized_dense (Array(30, 150447)) 'X dict vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.root.X_feature_hasher_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_feature_hasher_dense (Array(30, 262144)) 'X feature hasher vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Compressed `pytables`\n",
    "- `pytables` also exposes an `CArray` type that compresses the data\n",
    "- A number of compression algorithms are provided, including `zlib`, `blosc`, and `lzo`\n",
    "- Furthermore, memory can be optimized using `HD5`'s ability to handle in-memory processing via the `H5FD_CORE` driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This time, we'll create one table to store both arrays and we'll use `blosc` for\n",
    "# compression\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode=\"w\")\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "X_dict_vectorized_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_dict_vectorized_CArray',\n",
    "                                    tables.Atom.from_dtype(X_dict_vectorized.dtype),\n",
    "                                    shape=X_dict_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_dict_vectorized_CArray[:] = X_dict_vectorized.todense()\n",
    "X_feature_hasher_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_feature_hasher_CArray',\n",
    "                                    tables.Atom.from_dtype(X_feature_hasher_vectorized.dtype),\n",
    "                                    shape=X_feature_hasher_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_feature_hasher_CArray[:] = X_feature_hasher_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the size of `X_compressed.h5`, which contains both arrays, is only 8.6 MB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's read in the data from the table with the `H5FD_CORE` driver and train\n",
    "# a model with it\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode='r', driver='H5FD_CORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_CArray = X_compressed_file.root.X_dict_vectorized_CArray\n",
    "X_feature_hasher_CArray = X_compressed_file.root.X_dict_vectorized_CArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1 = Perceptron()\n",
    "perc2 = Perceptron()\n",
    "perc1.fit(X_dict_vectorized_CArray, y)\n",
    "perc1.fit(X_feature_hasher_CArray, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Compressed AND Extendable `pytables`\n",
    "- `pytables` provides a further type of array that is both compressed and extendable, i.e., `EArray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split up our data into 3 parts\n",
    "data_ids_parts = []\n",
    "for i in range(0, len(data_ids), 10):\n",
    "    data_ids_parts.append(data_ids[i:i + 10])\n",
    "[len(data_) for data_ in data_ids_parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will build up the `FeatureHasher` vectorizer first and in such a way that all\n",
    "# of the data does not need to be held on disk at one time\n",
    "vec = FeatureHasher(n_features=2**18, non_negative=True)\n",
    "for data_ids_part in data_ids_parts:\n",
    "    X_ = []\n",
    "    for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids_part):\n",
    "        sample = get_data_point(doc, prediction_label=label,\n",
    "                                non_nlp_features=non_nlp_feature_set_labels,\n",
    "                                bin_ranges=bin_ranges)\n",
    "        if not sample: continue\n",
    "        X_.append(sample.get('x', sample))\n",
    "    vec.fit(X_)\n",
    "del X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's find out what the expected dtype/shape is supposed to be\n",
    "X_ = []\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids[:5]):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    X_.append(sample.get('x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = vec.transform(X_).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that the vectorizer is fully fit, we can now read in the data\n",
    "# incrementally, transform it, and save it to disk in a table\n",
    "X_compressed_extendable_file = tables.open_file(\"X_compressed_extendable.h5\", mode=\"w\")\n",
    "filters = tables.Filters(complevel=5, complib='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array\n",
    "X_earray = (X_compressed_extendable_file\n",
    "            .create_earray(X_compressed_extendable_file.root,\n",
    "                           'X_earray',\n",
    "                           tables.Atom.from_dtype(X_.dtype),\n",
    "                           shape=(0, X_.shape[1]),\n",
    "                           filters=filters,\n",
    "                           expectedrows=len(data_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_earray (EArray(0, 262144), shuffle, blosc(5)) ''\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1, 16384)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_earray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data_ids_part in data_ids_parts:\n",
    "    X_part = []\n",
    "    for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids_part):\n",
    "        sample = get_data_point(doc, prediction_label=label,\n",
    "                                non_nlp_features=non_nlp_feature_set_labels,\n",
    "                                bin_ranges=bin_ranges)\n",
    "        if not sample: continue\n",
    "        X_part.append(sample.get('x'))\n",
    "    X_earray.append(vec.transform(X_part).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append rows to `X_earray` one at a time\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    X_ = vec.transform([sample.get('x')]).todense()\n",
    "    X_earray.append(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_earray (EArray(60, 262144), shuffle, blosc(5)) ''\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1, 16384)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_earray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mmulholland mmulholland 16M Mar  8 00:18 X_compressed_extendable.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh X_compressed_extendable.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_extendable_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_extendable_file = tables.open_file(\"X_compressed_extendable.h5\",\n",
    "                                                mode='r',\n",
    "                                                driver='H5FD_CORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another way of referring to a specific dataset within a file (i.e., a \"node\"\n",
    "# under the \"root\" of the file-system)\n",
    "X_earray = X_compressed_extendable_file.get_node('/', 'X_earray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  1., ...,  2.,  0.,  1.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_earray[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_extendable_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
