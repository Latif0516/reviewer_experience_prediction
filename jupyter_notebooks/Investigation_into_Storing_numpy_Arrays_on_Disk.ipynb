{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of Methods of Storing Data in `numpy` Arrays on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "import tables\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction import (FeatureHasher,\n",
    "                                        DictVectorizer)\n",
    "\n",
    "from src import parse_non_nlp_features_string\n",
    "from src.mongodb import connect_to_db\n",
    "from src.experiments import (make_cursor,\n",
    "                             get_data_point,\n",
    "                             ExperimentalData)\n",
    "from src.datasets import get_bin_ranges_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to reviews database\n",
    "db = connect_to_db(host='localhost', port=37017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n"
     ]
    }
   ],
   "source": [
    "# Get some data for \"Dota_2\" with the label \"total_game_hours\" and with\n",
    "# the number of bins set to 3 and the bin factor set to 6.0\n",
    "game = 'Dota_2'\n",
    "label = 'total_game_hours'\n",
    "nbins = 3\n",
    "bin_factor = 6.0\n",
    "#bin_ranges = get_bin_ranges_helper(db, [game], label, nbins, bin_factor)\n",
    "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n",
    "print(\"bin_ranges = {}\".format(bin_ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = ExperimentalData(db=db, prediction_label=label, games=[game],\n",
    "                        folds=1, fold_size=30, grid_search_folds=0,\n",
    "                        grid_search_fold_size=0, bin_ranges=bin_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['5690a60fe76db81bef5c4613', '5690a60fe76db81bef5c30ed',\n",
       "        '5690a60fe76db81bef5c322d', '5690a60fe76db81bef5c49ce',\n",
       "        '5690a60fe76db81bef5c28ce', '5690a60fe76db81bef5c44b0',\n",
       "        '5690a60fe76db81bef5c4987', '5690a60fe76db81bef5c496c',\n",
       "        '5690a60fe76db81bef5c3e3f', '5690a60fe76db81bef5c4b13',\n",
       "        '5690a60fe76db81bef5c403e', '5690a60fe76db81bef5c3cec',\n",
       "        '5690a60fe76db81bef5c2e06', '5690a60fe76db81bef5c4464',\n",
       "        '5690a60fe76db81bef5c3fd2', '5690a60fe76db81bef5c3510',\n",
       "        '5690a60fe76db81bef5c36bb', '5690a60fe76db81bef5c2f74',\n",
       "        '5690a60fe76db81bef5c2f51', '5690a60fe76db81bef5c4479',\n",
       "        '5690a60fe76db81bef5c4ad5', '5690a60fe76db81bef5c47ff',\n",
       "        '5690a60fe76db81bef5c3f2a', '5690a60fe76db81bef5c3872',\n",
       "        '5690a60fe76db81bef5c2a3f', '5690a60fe76db81bef5c4465',\n",
       "        '5690a60fe76db81bef5c2750', '5690a60fe76db81bef5c46c6',\n",
       "        '5690a60fe76db81bef5c2aba', '5690a60fe76db81bef5c3973'], \n",
       "       dtype='<U24')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have a set of IDs to work with, which point to samples in the\n",
    "# dataset\n",
    "data.training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5690a60fe76db81bef5c4613',\n",
       " '5690a60fe76db81bef5c30ed',\n",
       " '5690a60fe76db81bef5c322d',\n",
       " '5690a60fe76db81bef5c49ce',\n",
       " '5690a60fe76db81bef5c28ce',\n",
       " '5690a60fe76db81bef5c44b0',\n",
       " '5690a60fe76db81bef5c4987',\n",
       " '5690a60fe76db81bef5c496c',\n",
       " '5690a60fe76db81bef5c3e3f',\n",
       " '5690a60fe76db81bef5c4b13']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = list(data.training_set[0])\n",
    "data_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'friend_player_level', 'num_screenshots', 'num_found_unhelpful', 'found_helpful_percentage', 'num_games_owned', 'num_comments', 'num_voted_helpfulness', 'num_achievements_possible', 'num_guides', 'num_found_helpful', 'num_badges', 'num_achievements_attained', 'num_groups', 'num_found_funny', 'num_achievements_percentage', 'num_workshop_items', 'num_reviews', 'num_friends'}\n"
     ]
    }
   ],
   "source": [
    "# For our features, we will use all of the NLP features + the review/reviewer\n",
    "# attributes that are not directly related to the label (\"total_game_hours\")\n",
    "non_nlp_feature_set_labels = parse_non_nlp_features_string(\"all\", label)\n",
    "print(non_nlp_feature_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll get the actual labels and the corresponding features for each\n",
    "# sample\n",
    "y = []\n",
    "X = []\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    y.append(sample.get('y', sample))\n",
    "    X.append(sample.get('x', sample))\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 1, 2, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example labels\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yea awk', 1),\n",
       " ('stupidity', 1),\n",
       " ('badge ,', 1),\n",
       " ('legislation that', 1),\n",
       " ('other features', 1),\n",
       " ('thana', 1),\n",
       " ('would like', 1),\n",
       " ('ranked matches', 1),\n",
       " ('in comments', 1),\n",
       " ('my virgin', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example features (sparse format)\n",
    "list(X[0].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll make a vectorizer object (actually, two vectors, one with `DictVectorizer`\n",
    "# and the other with `FeatureHasher`) and fit with `X`\n",
    "dict_vec = DictVectorizer(sparse=True)\n",
    "feature_hasher_vec = FeatureHasher(n_features=2**18, non_negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vec.fit(X)\n",
    "feature_hasher_vec.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we must transform `X` with the vectorizers to get the sparse scipy arrays\n",
    "X_dict_vectorized = dict_vec.transform(X)\n",
    "X_feature_hasher_vectorized = feature_hasher_vec.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30x150447 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4223615 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_dict_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_feature_hasher_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,  140430,  280869,  421947,  562705,  706627,  847056,\n",
       "        987834, 1128189, 1268661, 1409102, 1549611, 1689989, 1830817,\n",
       "       1971240, 2111773, 2252133, 2392525, 2532868, 2673419, 2814388,\n",
       "       2954836, 3096452, 3237277, 3378794, 3521468, 3662042, 3802450,\n",
       "       3942804, 4083194, 4223615], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,  108600,  217212,  326169,  434956,  545560,  654160,\n",
       "        762970,  871527,  980153, 1088762, 1197416, 1305980, 1414819,\n",
       "       1523405, 1632054, 1740618, 1849192, 1957736, 2066414, 2175328,\n",
       "       2283926, 2393221, 2502043, 2611281, 2721157, 2829832, 2938418,\n",
       "       3046972, 3155548, 3264138], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_vectorized.indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `pytables` to Store `scipy` Arrays to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scipy` sparse arrays unfortunately cannot be stored with `pytables`; however, they can be converted to dense arrays and then stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open new empty HDF5 files\n",
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\", mode=\"w\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the root groups\n",
    "root_dict_vectorized = X_dict_vectorized_dense_file.root\n",
    "root_feature_hasher_vectorized = X_feature_hasher_dense_file.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the dense arrays on the HDF5 files\n",
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.create_array(root_dict_vectorized,\n",
    "                                              'X_dict_vectorized_dense',\n",
    "                                              X_dict_vectorized.todense(),\n",
    "                                              \"X dict vectorized dense\")\n",
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.create_array(root_feature_hasher_vectorized,\n",
    "                                             'X_feature_hasher_dense',\n",
    "                                             X_feature_hasher_vectorized.todense(),\n",
    "                                             \"X feature hasher vectorized dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mmulholland mmulholland 35M Feb 29 22:24 X_dict_vectorized_dense.h5\r\n",
      "-rw-rw-r-- 1 mmulholland mmulholland 61M Feb 29 22:24 X_feature_hasher_dense.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh *h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, to store even a relatively small 30-sample dataset vectorized with `DictVectorizer` in a dense format, it can be 35 MB\n",
    "- Suprisingly, storing the same dataset vectorized with `FeatureHasher`, which is supposed to be memory-efficient, requires even more memory to store the same data (61 MB) in a dense format\n",
    "- Let's see what we can do with the array that's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_dict_vectorized_dense (Array(30, 150447)) 'X dict vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_feature_hasher_dense (Array(30, 262144)) 'X feature hasher vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can a model be trained with `pytables` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc1 = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.predict(X_dict_vectorized_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2 = Perceptron()\n",
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2.predict(X_feature_hasher_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, it seems that `pytables` data can be used as a drop-in replacement for non-sparse `numpy` arrays (or `todense`-converted sparse `scipy` arrays as generated via `DictVectorizer`/`FeatureHasher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to use `pytables` in order to increase memory efficiency, a possible algorithm would be the following:\n",
    "    - Extract and vectorize data\n",
    "    - Use `todense` to make the arrays dense\n",
    "    - Save to data an `hdf5` file with `pytables`\n",
    "    - Remove the original data so that it gets garbage-collected\n",
    "    - Use `pytables` arrays in place of data wherever needed\n",
    "    - Remove the `hdf5` files after complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that it is possible to create enlargeable arrays with `pytables`, so it's possible that an array file could be generated, saved, and closed, and then reopened and enlargened and stored again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the `hdf5` files have been created, arrays saved to them, and then closed, let's try to read in the data again and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.root.X_dict_vectorized_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_dict_vectorized_dense (Array(30, 150447)) 'X dict vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.root.X_feature_hasher_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_feature_hasher_dense (Array(30, 262144)) 'X feature hasher vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Compressed `pytables`\n",
    "- `pytables` also exposes an `CArray` type that compresses the data\n",
    "- A number of compression algorithms are provided, including `zlib`, `blosc`, and `lzo`\n",
    "- Furthermore, memory can be optimized using `HD5`'s ability to handle in-memory processing via the `H5FD_CORE` driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This time, we'll create one table to store both arrays and we'll use `blosc` for\n",
    "# compression\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode=\"w\")\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "X_dict_vectorized_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_dict_vectorized_CArray',\n",
    "                                    tables.Atom.from_dtype(X_dict_vectorized.dtype),\n",
    "                                    shape=X_dict_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_dict_vectorized_CArray[:] = X_dict_vectorized.todense()\n",
    "X_feature_hasher_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_feature_hasher_CArray',\n",
    "                                    tables.Atom.from_dtype(X_feature_hasher_vectorized.dtype),\n",
    "                                    shape=X_feature_hasher_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_feature_hasher_CArray[:] = X_feature_hasher_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the size of `X_compressed.h5`, which contains both arrays, is only 8.6 MB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's read in the data from the table with the `H5FD_CORE` driver and train\n",
    "# a model with it\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode='r', driver='H5FD_CORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_CArray = X_compressed_file.root.X_dict_vectorized_CArray\n",
    "X_feature_hasher_CArray = X_compressed_file.root.X_dict_vectorized_CArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1 = Perceptron()\n",
    "perc2 = Perceptron()\n",
    "perc1.fit(X_dict_vectorized_CArray, y)\n",
    "perc1.fit(X_feature_hasher_CArray, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
