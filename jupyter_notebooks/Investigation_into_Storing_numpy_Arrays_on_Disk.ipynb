{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of Methods of Storing Data in `numpy` Arrays on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove h5 files if they exist\n",
    "! rm -f *h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "import tables\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from itertools import chain\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction import (FeatureHasher,\n",
    "                                        DictVectorizer)\n",
    "\n",
    "from src import parse_non_nlp_features_string\n",
    "from src.mongodb import connect_to_db\n",
    "from src.experiments import (make_cursor,\n",
    "                             get_data_point,\n",
    "                             ExperimentalData)\n",
    "from src.datasets import get_bin_ranges_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to reviews database\n",
    "db = connect_to_db(host='localhost', port=37017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n"
     ]
    }
   ],
   "source": [
    "# Get some data for \"Dota_2\" with the label \"total_game_hours\" and with\n",
    "# the number of bins set to 3 and the bin factor set to 6.0\n",
    "game = 'Dota_2'\n",
    "label = 'total_game_hours'\n",
    "nbins = 3\n",
    "bin_factor = 6.0\n",
    "#bin_ranges = get_bin_ranges_helper(db, [game], label, nbins, bin_factor)\n",
    "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n",
    "print(\"bin_ranges = {}\".format(bin_ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = ExperimentalData(db=db, prediction_label=label, games=[game],\n",
    "                        folds=1, fold_size=30, grid_search_folds=0,\n",
    "                        grid_search_fold_size=0, bin_ranges=bin_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['5690a60fe76db81bef5c4613', '5690a60fe76db81bef5c30ed',\n",
       "        '5690a60fe76db81bef5c322d', '5690a60fe76db81bef5c49ce',\n",
       "        '5690a60fe76db81bef5c28ce', '5690a60fe76db81bef5c44b0',\n",
       "        '5690a60fe76db81bef5c4987', '5690a60fe76db81bef5c496c',\n",
       "        '5690a60fe76db81bef5c3e3f', '5690a60fe76db81bef5c4b13',\n",
       "        '5690a60fe76db81bef5c403e', '5690a60fe76db81bef5c3cec',\n",
       "        '5690a60fe76db81bef5c2e06', '5690a60fe76db81bef5c4464',\n",
       "        '5690a60fe76db81bef5c3fd2', '5690a60fe76db81bef5c3510',\n",
       "        '5690a60fe76db81bef5c36bb', '5690a60fe76db81bef5c2f74',\n",
       "        '5690a60fe76db81bef5c2f51', '5690a60fe76db81bef5c4479',\n",
       "        '5690a60fe76db81bef5c4ad5', '5690a60fe76db81bef5c47ff',\n",
       "        '5690a60fe76db81bef5c3f2a', '5690a60fe76db81bef5c3872',\n",
       "        '5690a60fe76db81bef5c2a3f', '5690a60fe76db81bef5c4465',\n",
       "        '5690a60fe76db81bef5c2750', '5690a60fe76db81bef5c46c6',\n",
       "        '5690a60fe76db81bef5c2aba', '5690a60fe76db81bef5c3973'], \n",
       "       dtype='<U24')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have a set of IDs to work with, which point to samples in the\n",
    "# dataset\n",
    "data.training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5690a60fe76db81bef5c4613',\n",
       " '5690a60fe76db81bef5c30ed',\n",
       " '5690a60fe76db81bef5c322d',\n",
       " '5690a60fe76db81bef5c49ce',\n",
       " '5690a60fe76db81bef5c28ce',\n",
       " '5690a60fe76db81bef5c44b0',\n",
       " '5690a60fe76db81bef5c4987',\n",
       " '5690a60fe76db81bef5c496c',\n",
       " '5690a60fe76db81bef5c3e3f',\n",
       " '5690a60fe76db81bef5c4b13']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = list(data.training_set[0])\n",
    "data_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_achievements_percentage', 'num_found_unhelpful', 'num_comments', 'num_workshop_items', 'num_screenshots', 'num_groups', 'num_voted_helpfulness', 'num_found_helpful', 'num_reviews', 'num_games_owned', 'found_helpful_percentage', 'friend_player_level', 'num_badges', 'num_found_funny', 'num_friends', 'num_achievements_possible', 'num_guides', 'num_achievements_attained'}\n"
     ]
    }
   ],
   "source": [
    "# For our features, we will use all of the NLP features + the review/reviewer\n",
    "# attributes that are not directly related to the label (\"total_game_hours\")\n",
    "non_nlp_feature_set_labels = parse_non_nlp_features_string(\"all\", label)\n",
    "print(non_nlp_feature_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll get the actual labels and the corresponding features for each\n",
    "# sample\n",
    "y = []\n",
    "X = []\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    y.append(sample.get('y'))\n",
    "    X.append(sample.get('x'))\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 1, 2, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example labels\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('community coupled', 1),\n",
       " ('any child', 1),\n",
       " ('show that', 1),\n",
       " ('long catch', 1),\n",
       " (\"n't respect\", 1),\n",
       " ('doing sport', 1),\n",
       " (\"n't needed\", 1),\n",
       " ('served', 1),\n",
       " ('barely listen', 1),\n",
       " ('automated', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example features (sparse format)\n",
    "list(X[0].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.007722007722007722,\n",
       " 0.031746031746031744,\n",
       " 0.03571428571428571,\n",
       " 0.09615384615384616,\n",
       " 0.16216216216216217,\n",
       " 0.16666666666666666,\n",
       " 0.34782608695652173,\n",
       " 0.5347490347490348,\n",
       " 0.5454545454545454,\n",
       " 0.625,\n",
       " 0.6521739130434783,\n",
       " 0.6666666666666666,\n",
       " 0.6931818181818182,\n",
       " 0.7142857142857143,\n",
       " 0.7176470588235294,\n",
       " 0.718562874251497,\n",
       " 0.75,\n",
       " 0.7659574468085106,\n",
       " 0.7904191616766467,\n",
       " 0.8,\n",
       " 0.9003164556962026,\n",
       " 0.9105398457583548,\n",
       " 0.9655172413793104,\n",
       " 0.9820359281437125,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 23,\n",
       " 24,\n",
       " 27,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 36,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 43,\n",
       " 44,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 61,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 69,\n",
       " 70,\n",
       " 80,\n",
       " 85,\n",
       " 88,\n",
       " 90,\n",
       " 97,\n",
       " 107,\n",
       " 120,\n",
       " 127,\n",
       " 132,\n",
       " 136,\n",
       " 141,\n",
       " 145,\n",
       " 147,\n",
       " 156,\n",
       " 164,\n",
       " 167,\n",
       " 169,\n",
       " 174,\n",
       " 182,\n",
       " 183,\n",
       " 203,\n",
       " 217,\n",
       " 277,\n",
       " 288,\n",
       " 295,\n",
       " 333,\n",
       " 360,\n",
       " 518,\n",
       " 569,\n",
       " 632,\n",
       " 1771,\n",
       " 1945}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types of values found in data\n",
    "# Perhaps they can be represented instead with a less precise data-type than\n",
    "# the current (float64)\n",
    "set(chain(*[[val for _, val in x.items()] for x in X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll make a vectorizer object (actually, two vectors, one with `DictVectorizer`\n",
    "# and the other with `FeatureHasher`) and fit with `X`\n",
    "dict_vec = DictVectorizer(sparse=True)\n",
    "feature_hasher_vec = FeatureHasher(n_features=2**18, non_negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vec.fit(X)\n",
    "feature_hasher_vec.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_hasher_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we must transform `X` with the vectorizers to get the sparse scipy arrays\n",
    "X_dict_vectorized = dict_vec.transform(X)\n",
    "X_feature_hasher_vectorized = feature_hasher_vec.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30x150447 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4223615 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_dict_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_feature_hasher_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,  140430,  280869,  421947,  562705,  706627,  847056,\n",
       "        987834, 1128189, 1268661, 1409102, 1549611, 1689989, 1830817,\n",
       "       1971240, 2111773, 2252133, 2392525, 2532868, 2673419, 2814388,\n",
       "       2954836, 3096452, 3237277, 3378794, 3521468, 3662042, 3802450,\n",
       "       3942804, 4083194, 4223615], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,  108600,  217212,  326169,  434956,  545560,  654160,\n",
       "        762970,  871527,  980153, 1088762, 1197416, 1305980, 1414819,\n",
       "       1523405, 1632054, 1740618, 1849192, 1957736, 2066414, 2175328,\n",
       "       2283926, 2393221, 2502043, 2611281, 2721157, 2829832, 2938418,\n",
       "       3046972, 3155548, 3264138], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_vectorized.indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `pytables` to Store `scipy` Arrays to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scipy` sparse arrays unfortunately cannot be stored with `pytables`; however, they can be converted to dense arrays and then stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open new empty HDF5 files\n",
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\", mode=\"w\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the root groups\n",
    "root_dict_vectorized = X_dict_vectorized_dense_file.root\n",
    "root_feature_hasher_vectorized = X_feature_hasher_dense_file.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the dense arrays on the HDF5 files\n",
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.create_array(root_dict_vectorized,\n",
    "                                              'X_dict_vectorized_dense',\n",
    "                                              X_dict_vectorized.todense(),\n",
    "                                              \"X dict vectorized dense\")\n",
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.create_array(root_feature_hasher_vectorized,\n",
    "                                             'X_feature_hasher_dense',\n",
    "                                             X_feature_hasher_vectorized.todense(),\n",
    "                                             \"X feature hasher vectorized dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mmulholland mmulholland 35M Mar  8 14:53 X_dict_vectorized_dense.h5\r\n",
      "-rw-rw-r-- 1 mmulholland mmulholland 61M Mar  8 14:53 X_feature_hasher_dense.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh X*_dense.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, to store even a relatively small 30-sample dataset vectorized with `DictVectorizer` in a dense format, it can be 35 MB\n",
    "- Suprisingly, storing the same dataset vectorized with `FeatureHasher`, which is supposed to be memory-efficient, requires even more memory to store the same data (61 MB) in a dense format\n",
    "- Let's see what we can do with the array that's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_dict_vectorized_dense (Array(30, 150447)) 'X dict vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_feature_hasher_dense (Array(30, 262144)) 'X feature hasher vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can a model be trained with `pytables` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc1 = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.predict(X_dict_vectorized_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2 = Perceptron()\n",
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2.predict(X_feature_hasher_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, it seems that `pytables` data can be used as a drop-in replacement for non-sparse `numpy` arrays (or `todense`-converted sparse `scipy` arrays as generated via `DictVectorizer`/`FeatureHasher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to use `pytables` in order to increase memory efficiency, a possible algorithm would be the following:\n",
    "    - Extract and vectorize data\n",
    "    - Use `todense` to make the arrays dense\n",
    "    - Save to data an `hdf5` file with `pytables`\n",
    "    - Remove the original data so that it gets garbage-collected\n",
    "    - Use `pytables` arrays in place of data wherever needed\n",
    "    - Remove the `hdf5` files after complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that it is possible to create enlargeable arrays with `pytables`, so it's possible that an array file could be generated, saved, and closed, and then reopened and enlargened and stored again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the `hdf5` files have been created, arrays saved to them, and then closed, let's try to read in the data again and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.root.X_dict_vectorized_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_dict_vectorized_dense (Array(30, 150447)) 'X dict vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.root.X_feature_hasher_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_feature_hasher_dense (Array(30, 262144)) 'X feature hasher vectorized dense'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Compressed `pytables`\n",
    "- `pytables` also exposes an `CArray` type that compresses the data\n",
    "- A number of compression algorithms are provided, including `zlib`, `blosc`, and `lzo`\n",
    "- Furthermore, memory can be optimized using `HD5`'s ability to handle in-memory processing via the `H5FD_CORE` driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "row, column, and data array must all be the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-270bf2b64609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                                     \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_feature_hasher_vectorized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                     filters=filters)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX_feature_hasher_CArray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_feature_hasher_vectorized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \"\"\"\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtocoo\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;34m\"\"\" Checks data structure for consistency \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mnnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;31m# index arrays should have integer data types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mgetnnz\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mnnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                 raise ValueError('row, column, and data array must all be the '\n\u001b[0m\u001b[0;32m    221\u001b[0m                                  'same length')\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: row, column, and data array must all be the same length"
     ]
    }
   ],
   "source": [
    "# This time, we'll create one table to store both arrays and we'll use `blosc` for\n",
    "# compression\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode=\"w\")\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "X_dict_vectorized_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_dict_vectorized_CArray',\n",
    "                                    tables.Atom.from_dtype(X_dict_vectorized.dtype),\n",
    "                                    shape=X_dict_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_dict_vectorized_CArray[:] = X_dict_vectorized.todense()\n",
    "X_feature_hasher_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_feature_hasher_CArray',\n",
    "                                    tables.Atom.from_dtype(X_feature_hasher_vectorized.dtype),\n",
    "                                    shape=X_feature_hasher_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_feature_hasher_CArray[:] = X_feature_hasher_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the size of `X_compressed.h5`, which contains both arrays, is only 8.6 MB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's read in the data from the table with the `H5FD_CORE` driver and train\n",
    "# a model with it\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode='r', driver='H5FD_CORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_CArray = X_compressed_file.root.X_dict_vectorized_CArray\n",
    "X_feature_hasher_CArray = X_compressed_file.root.X_dict_vectorized_CArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc1 = Perceptron()\n",
    "perc2 = Perceptron()\n",
    "perc1.fit(X_dict_vectorized_CArray, y)\n",
    "perc1.fit(X_feature_hasher_CArray, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Compressed AND Extendable `pytables`\n",
    "- `pytables` provides a further type of array that is both compressed and extendable, i.e., `EArray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split up our data into 3 parts\n",
    "data_ids_parts = []\n",
    "for i in range(0, len(data_ids), 10):\n",
    "    data_ids_parts.append(data_ids[i:i + 10])\n",
    "[len(data_) for data_ in data_ids_parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will build up the `FeatureHasher` vectorizer first and in such a way that all\n",
    "# of the data does not need to be held on disk at one time\n",
    "vec = FeatureHasher(n_features=2**18, non_negative=True)\n",
    "for data_ids_part in data_ids_parts:\n",
    "    X_ = []\n",
    "    for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids_part):\n",
    "        sample = get_data_point(doc, prediction_label=label,\n",
    "                                non_nlp_features=non_nlp_feature_set_labels,\n",
    "                                bin_ranges=bin_ranges)\n",
    "        if not sample: continue\n",
    "        X_.append(sample.get('x', sample))\n",
    "    vec.fit(X_)\n",
    "del X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureHasher(dtype=<class 'numpy.float64'>, input_type='dict',\n",
       "       n_features=262144, non_negative=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's find out what the expected dtype/shape is supposed to be\n",
    "X_ = []\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids[:5]):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    X_.append(sample.get('x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = vec.transform(X_).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that the vectorizer is fully fit, we can now read in the data\n",
    "# incrementally, transform it, and save it to disk in a table\n",
    "X_compressed_extendable_file = tables.open_file(\"X_compressed_extendable.h5\", mode=\"w\")\n",
    "filters = tables.Filters(complevel=5, complib='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array\n",
    "X_earray = (X_compressed_extendable_file\n",
    "            .create_earray(X_compressed_extendable_file.root,\n",
    "                           'X_earray',\n",
    "                           tables.Atom.from_dtype(X_.dtype),\n",
    "                           shape=(0, X_.shape[1]),\n",
    "                           filters=filters,\n",
    "                           expectedrows=len(data_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_earray (EArray(0, 262144), shuffle, blosc(5)) ''\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1, 16384)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_earray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data_ids_part in data_ids_parts:\n",
    "    X_part = []\n",
    "    for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids_part):\n",
    "        sample = get_data_point(doc, prediction_label=label,\n",
    "                                non_nlp_features=non_nlp_feature_set_labels,\n",
    "                                bin_ranges=bin_ranges)\n",
    "        if not sample: continue\n",
    "        X_part.append(sample.get('x'))\n",
    "    X_earray.append(vec.transform(X_part).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append rows to `X_earray` one at a time\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    X_ = vec.transform([sample.get('x')]).todense()\n",
    "    X_earray.append(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/X_earray (EArray(60, 262144), shuffle, blosc(5)) ''\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1, 16384)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_earray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mmulholland mmulholland 16M Mar  8 14:54 X_compressed_extendable.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh X_compressed_extendable.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_extendable_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_extendable_file = tables.open_file(\"X_compressed_extendable.h5\",\n",
    "                                                mode='r',\n",
    "                                                driver='H5FD_CORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another way of referring to a specific dataset within a file (i.e., a \"node\"\n",
    "# under the \"root\" of the file-system)\n",
    "X_earray = X_compressed_extendable_file.get_node('/', 'X_earray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  1., ...,  2.,  0.,  1.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_earray[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_extendable_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
