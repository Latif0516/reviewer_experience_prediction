{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of Methods of Storing Data in `numpy` Arrays on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "import tables\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction import (FeatureHasher,\n",
    "                                        DictVectorizer)\n",
    "\n",
    "from src import parse_non_nlp_features_string\n",
    "from src.mongodb import connect_to_db\n",
    "from src.experiments import (make_cursor,\n",
    "                             get_data_point,\n",
    "                             ExperimentalData)\n",
    "from src.datasets import get_bin_ranges_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 9 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 8 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 7 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 6 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 5 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 4 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 3 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 2 more times...\n",
      "WARNING:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017. Will try 1 more time...\n",
      "ERROR:src.mongodb:Unable to connect client to Mongo server at mongodb://localhost:37017.\n"
     ]
    },
    {
     "ename": "ConnectionFailure",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAutoReconnect\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, host, port, max_pool_size, document_class, tz_aware, _connect, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAutoReconnect\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_ensure_connected\u001b[1;34m(self, sync)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \"\"\"\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__ensure_member\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m__ensure_member\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    812\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m                     \u001b[0mmember\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__find_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mmember\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m__find_node\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[1;31m# Couldn't find a suitable host.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAutoReconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAutoReconnect\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionFailure\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dc3458b67188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Connect to reviews database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnect_to_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'localhost'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m37017\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mmulholland/Documents/reviewer_experience_prediction/src/mongodb.pyx\u001b[0m in \u001b[0;36msrc.mongodb.connect_to_db (src/mongodb.c:1458)\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m                 logerr('Unable to connect client to Mongo server at {0}.'\n\u001b[0;32m     83\u001b[0m                        .format(mongo_url))\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 logwarn('Unable to connect client to Mongo server at {0}. '\n",
      "\u001b[1;32m/home/mmulholland/Documents/reviewer_experience_prediction/src/mongodb.pyx\u001b[0m in \u001b[0;36msrc.mongodb.connect_to_db (src/mongodb.c:1319)\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mtries\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             connection = MongoClient(mongo_url, max_pool_size=None,\n\u001b[0m\u001b[0;32m     78\u001b[0m                                      \u001b[0mconnectTimeoutMS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                                      socketKeepAlive=True)\n",
      "\u001b[1;32m/usr/local/share/conda/envs/reviews/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, host, port, max_pool_size, document_class, tz_aware, _connect, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAutoReconnect\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                 \u001b[1;31m# ConnectionFailure makes more sense here than AutoReconnect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mConnectionFailure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0musername\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionFailure\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Connect to reviews database\n",
    "db = connect_to_db(host='localhost', port=37017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get some data for \"Dota_2\" with the label \"total_game_hours\" and with\n",
    "# the number of bins set to 3 and the bin factor set to 6.0\n",
    "game = 'Dota_2'\n",
    "label = 'total_game_hours'\n",
    "nbins = 3\n",
    "bin_factor = 6.0\n",
    "#bin_ranges = get_bin_ranges_helper(db, [game], label, nbins, bin_factor)\n",
    "bin_ranges = [(0.0, 382.2), (382.3, 2675.5), (2675.6, 16435.0)]\n",
    "print(\"bin_ranges = {}\".format(bin_ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = ExperimentalData(db=db, prediction_label=label, games=[game],\n",
    "                        folds=1, fold_size=30, grid_search_folds=0,\n",
    "                        grid_search_fold_size=0, bin_ranges=bin_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we have a set of IDs to work with, which point to samples in the\n",
    "# dataset\n",
    "data.training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ids = list(data.training_set[0])\n",
    "data_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For our features, we will use all of the NLP features + the review/reviewer\n",
    "# attributes that are not directly related to the label (\"total_game_hours\")\n",
    "non_nlp_feature_set_labels = parse_non_nlp_features_string(\"all\", label)\n",
    "print(non_nlp_feature_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll get the actual labels and the corresponding features for each\n",
    "# sample\n",
    "y = []\n",
    "X = []\n",
    "for doc in make_cursor(db, projection={'_id': 0}, id_strings=data_ids):\n",
    "    sample = get_data_point(doc, prediction_label=label,\n",
    "                            non_nlp_features=non_nlp_feature_set_labels,\n",
    "                            bin_ranges=bin_ranges)\n",
    "    if not sample: continue\n",
    "    y.append(sample.get('y', sample))\n",
    "    X.append(sample.get('x', sample))\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example labels\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example features (sparse format)\n",
    "list(X[0].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll make a vectorizer object (actually, two vectors, one with `DictVectorizer`\n",
    "# and the other with `FeatureHasher`) and fit with `X`\n",
    "dict_vec = DictVectorizer(sparse=True)\n",
    "feature_hasher_vec = FeatureHasher(n_features=2**18, non_negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_vec.fit(X)\n",
    "feature_hasher_vec.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we must transform `X` with the vectorizers to get the sparse scipy arrays\n",
    "X_dict_vectorized = dict_vec.transform(X)\n",
    "X_feature_hasher_vectorized = feature_hasher_vec.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_dict_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_feature_hasher_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_vectorized.indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `pytables` to Store `scipy` Arrays to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scipy` sparse arrays unfortunately cannot be stored with `pytables`; however, they can be converted to dense arrays and then stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open new empty HDF5 files\n",
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\", mode=\"w\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the root groups\n",
    "root_dict_vectorized = X_dict_vectorized_dense_file.root\n",
    "root_feature_hasher_vectorized = X_feature_hasher_dense_file.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the dense arrays on the HDF5 files\n",
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.create_array(root_dict_vectorized,\n",
    "                                              'X_dict_vectorized_dense',\n",
    "                                              X_dict_vectorized.todense(),\n",
    "                                              \"X dict vectorized dense\")\n",
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.create_array(root_feature_hasher_vectorized,\n",
    "                                             'X_feature_hasher_dense',\n",
    "                                             X_feature_hasher_vectorized.todense(),\n",
    "                                             \"X feature hasher vectorized dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls -lh X*_dense.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, to store even a relatively small 30-sample dataset vectorized with `DictVectorizer` in a dense format, it can be 35 MB\n",
    "- Suprisingly, storing the same dataset vectorized with `FeatureHasher`, which is supposed to be memory-efficient, requires even more memory to store the same data (61 MB) in a dense format\n",
    "- Let's see what we can do with the array that's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can a model be trained with `pytables` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc1 = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc1.predict(X_dict_vectorized_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc2 = Perceptron()\n",
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc2.predict(X_feature_hasher_dense_hdf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, it seems that `pytables` data can be used as a drop-in replacement for non-sparse `numpy` arrays (or `todense`-converted sparse `scipy` arrays as generated via `DictVectorizer`/`FeatureHasher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to use `pytables` in order to increase memory efficiency, a possible algorithm would be the following:\n",
    "    - Extract and vectorize data\n",
    "    - Use `todense` to make the arrays dense\n",
    "    - Save to data an `hdf5` file with `pytables`\n",
    "    - Remove the original data so that it gets garbage-collected\n",
    "    - Use `pytables` arrays in place of data wherever needed\n",
    "    - Remove the `hdf5` files after complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that it is possible to create enlargeable arrays with `pytables`, so it's possible that an array file could be generated, saved, and closed, and then reopened and enlargened and stored again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the `hdf5` files have been created, arrays saved to them, and then closed, let's try to read in the data again and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file = tables.open_file(\"X_dict_vectorized_dense.h5\")\n",
    "X_feature_hasher_dense_file = tables.open_file(\"X_feature_hasher_dense.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_hdf_array = \\\n",
    "    X_dict_vectorized_dense_file.root.X_dict_vectorized_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_dense_hdf_array = \\\n",
    "    X_feature_hasher_dense_file.root.X_feature_hasher_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_feature_hasher_dense_hdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc1.fit(X_dict_vectorized_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc2.fit(X_feature_hasher_dense_hdf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_dense_file.close()\n",
    "X_feature_hasher_dense_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Compressed `pytables`\n",
    "- `pytables` also exposes an `CArray` type that compresses the data\n",
    "- A number of compression algorithms are provided, including `zlib`, `blosc`, and `lzo`\n",
    "- Furthermore, memory can be optimized using `HD5`'s ability to handle in-memory processing via the `H5FD_CORE` driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This time, we'll create one table to store both arrays and we'll use `blosc` for\n",
    "# compression\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode=\"w\")\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "X_dict_vectorized_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_dict_vectorized_CArray',\n",
    "                                    tables.Atom.from_dtype(X_dict_vectorized.dtype),\n",
    "                                    shape=X_dict_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_dict_vectorized_CArray[:] = X_dict_vectorized.todense()\n",
    "X_feature_hasher_CArray = \\\n",
    "    X_compressed_file.create_carray(X_compressed_file.root,\n",
    "                                    'X_feature_hasher_CArray',\n",
    "                                    tables.Atom.from_dtype(X_feature_hasher_vectorized.dtype),\n",
    "                                    shape=X_feature_hasher_vectorized.shape,\n",
    "                                    filters=filters)\n",
    "X_feature_hasher_CArray[:] = X_feature_hasher_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the size of `X_compressed.h5`, which contains both arrays, is only 8.6 MB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's read in the data from the table with the `H5FD_CORE` driver and train\n",
    "# a model with it\n",
    "X_compressed_file = tables.open_file(\"X_compressed.h5\", mode='r', driver='H5FD_CORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict_vectorized_CArray = X_compressed_file.root.X_dict_vectorized_CArray\n",
    "X_feature_hasher_CArray = X_compressed_file.root.X_dict_vectorized_CArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc1 = Perceptron()\n",
    "perc2 = Perceptron()\n",
    "perc1.fit(X_dict_vectorized_CArray, y)\n",
    "perc1.fit(X_feature_hasher_CArray, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_compressed_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
