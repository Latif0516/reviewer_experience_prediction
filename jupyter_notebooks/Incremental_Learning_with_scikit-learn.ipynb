{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.features import *\n",
    "from util.datasets import *\n",
    "from util.mongodb import *\n",
    "\n",
    "import numpy as np\n",
    "from bson import BSON\n",
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans # Non-incremental learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_selection import (chi2,\n",
    "                                       SelectKBest)\n",
    "from sklearn.naive_bayes import (BernoulliNB,\n",
    "                                 MultinomialNB)\n",
    "from sklearn.linear_model import (Perceptron,\n",
    "                                  SGDRegressor,\n",
    "                                  SGDClassifier,\n",
    "                                  PassiveAggressiveRegressor,\n",
    "                                  PassiveAggressiveClassifier)\n",
    "from sklearn.decomposition import (IncrementalPCA,\n",
    "                                   MiniBatchDictionaryLearning)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import (r2_score,\n",
    "                             precision_score,\n",
    "                             f1_score,\n",
    "                             average_precision_score,\n",
    "                             accuracy_score,\n",
    "                             confusion_matrix)\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running Mongo server on port 2700 and running on my own personal server\n",
    "# ('localhost' in this case)\n",
    "host = 'localhost'\n",
    "port = 2700\n",
    "db = connect_to_db(host=host,\n",
    "                   port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54051"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training/test reviews across all games\n",
    "db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arma_3.jsonlines\r\n",
      "Counter_Strike_Global_Offensive.jsonlines\r\n",
      "Counter_Strike.jsonlines\r\n",
      "Dota_2.jsonlines\r\n",
      "Football_Manager_2015.jsonlines\r\n",
      "Garrys_Mod.jsonlines\r\n",
      "Grand_Theft_Auto_V.jsonlines\r\n",
      "sample.jsonlines\r\n",
      "Sid_Meiers_Civilization_5.jsonlines\r\n",
      "Team_Fortress_2.jsonlines\r\n",
      "The_Elder_Scrolls_V.jsonlines\r\n",
      "Warframe.jsonlines\r\n"
     ]
    }
   ],
   "source": [
    "# List games that the database contains data for\n",
    "! ls ../data/*jsonlines | awk -F/ '{print $NF}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('560394d3cbb14611d0957f1c'),\n",
       " 'achievement_progress': {'num_achievements_attained': 7,\n",
       "  'num_achievements_percentage': 0.16279069767441862,\n",
       "  'num_achievements_possible': 43},\n",
       " 'appid': '107410',\n",
       " 'bin_factor': 2.0,\n",
       " 'bin_ranges': [[0.0, 338.1], [338.2, 1014.4], [1014.5, 2367.0]],\n",
       " 'binarized': True,\n",
       " 'date_posted': 'Dec 15, 2013, 7:32PM',\n",
       " 'date_updated': None,\n",
       " 'found_helpful_percentage': 0.5,\n",
       " 'friend_player_level': 7,\n",
       " 'game': 'Arma_3',\n",
       " 'id_string': '560394d3cbb14611d0957f1c',\n",
       " 'nbins': 3,\n",
       " 'num_badges': 5,\n",
       " 'num_comments': 1,\n",
       " 'num_found_funny': 0,\n",
       " 'num_found_helpful': 2,\n",
       " 'num_found_unhelpful': 2,\n",
       " 'num_friends': 35,\n",
       " 'num_games_owned': 75,\n",
       " 'num_groups': 7,\n",
       " 'num_guides': 0,\n",
       " 'num_reviews': 1,\n",
       " 'num_screenshots': 789,\n",
       " 'num_voted_helpfulness': 4,\n",
       " 'num_workshop_items': 1,\n",
       " 'orig_url': 'http://steamcommunity.com/app/107410/homecontent/?userreviewsoffset=5150&p=1&itemspage=516&screenshotspage=516&videospage=516&artpage=516&allguidepage=516&webguidepage=516&integratedguidepage=516&discussionspage=516&appid=107410&appHubSubSection=10&appHubSubSection=10&l=english&browsefilter=toprated&filterLanguage=default&searchText=&forceanon=1',\n",
       " 'partition': 'training',\n",
       " 'profile_url': 'http://steamcommunity.com/id/EthanTheFinn',\n",
       " 'rating': 'Recommended',\n",
       " 'review': '10 - Graphics 9.5 - Sound 10 - Gameplay 9.7 - Voice Acting 10 - Potential (Modding, and custom mission scenarios) Overal this is a great game!',\n",
       " 'review_url': 'http://steamcommunity.com/id/EthanTheFinn/recommended/107410/',\n",
       " 'steam_id_number': 'EthanTheFinn',\n",
       " 'total_game_hours': 602.3,\n",
       " 'total_game_hours_bin': 2,\n",
       " 'total_game_hours_last_two_weeks': 0.0,\n",
       " 'username': 'Ethan'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get a sense for the kind of data that is contained in each document\n",
    "# (not including the NLP features, which have to be decoded, anyway)\n",
    "db.find_one({},\n",
    "            {'nlp_features': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['nbins', 'total_game_hours_bin', 'num_voted_helpfulness', '_id', 'friend_player_level', 'rating', 'bin_factor', 'num_workshop_items', 'orig_url', 'num_found_unhelpful', 'found_helpful_percentage', 'game', 'num_comments', 'appid', 'review_url', 'steam_id_number', 'achievement_progress', 'num_friends', 'profile_url', 'num_groups', 'binarized', 'date_updated', 'num_found_helpful', 'num_badges', 'num_reviews', 'num_games_owned', 'total_game_hours', 'num_found_funny', 'num_guides', 'username', 'total_game_hours_last_two_weeks', 'num_screenshots', 'partition', 'bin_ranges', 'id_string', 'review', 'date_posted'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature types\n",
    "db.find_one({},\n",
    "            {'nlp_features': 0}).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's use Arma 3\n",
    "game_id = 'Arma_3'\n",
    "# Create cursors for clustering/exploration and training/test sets\n",
    "# (limiting the test set to 500 for now)\n",
    "#dev_cursor = db.find({'game': game_id,\n",
    "#                      'partition': 'training'},\n",
    "#                     timeout=False)\n",
    "#dev_cursor.batch_size = 20\n",
    "train_cursor = db.find({'game': game_id,\n",
    "                        'partition': 'training'},\n",
    "                       timeout=False)\n",
    "train_cursor.batch_size = 20\n",
    "test_cursor = db.find({'game': game_id,\n",
    "                       'partition': 'test'},\n",
    "                      timeout=False).limit(500)\n",
    "test_cursor.batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make a training set of 100 reviews that includes all of the NLP\n",
    "# features + most of the other features like \"num_found_funny\", etc. and\n",
    "# where the thing we're trying to predict is 'total_game_hours_bin'.\n",
    "# We will also make a test set that consists of all test set review\n",
    "# documents.\n",
    "non_nlp_feature_types = ['num_guides', 'num_games_owned', 'num_friends',\n",
    "                         'num_voted_helpfulness', 'num_groups',\n",
    "                         'num_workshop_items', 'num_reviews',\n",
    "                         'num_found_funny', 'friend_player_level',\n",
    "                         'num_badges', 'num_found_helpful',\n",
    "                         'num_screenshots', 'num_found_unhelpful',\n",
    "                         'found_helpful_percentage', 'num_comments']\n",
    "hours_feature = 'total_game_hours_bin'\n",
    "_id = 'id_string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dev_data(training_data_cursor,\n",
    "                 inc_size,\n",
    "                 non_nlp_features_to_use):\n",
    "    '''\n",
    "    Get a list of development data dictionaries to use in dataset\n",
    "    exploration.\n",
    "\n",
    "    :param training_data_cursor: cursor for training partition\n",
    "                                 documents\n",
    "    :type training_data_cursor: pymongo.cursor.Cursor object\n",
    "    :param inc_size: number of documents to extract\n",
    "    :type inc_size: int\n",
    "    :param non_nlp_features_to_use: list of non-NLP features to add into\n",
    "                                    the feature dictionaries \n",
    "    :type non_nlp_features_to_use: list of str\n",
    "    :returns: list of dict\n",
    "    '''\n",
    "\n",
    "    if inc_size < 1:\n",
    "        raise ValueError('inc_size parameter should be positive integer')\n",
    "    data = []\n",
    "    i = 0\n",
    "    while i < inc_size:\n",
    "        try:\n",
    "            review_doc = next(training_data_cursor)\n",
    "        except StopIteration:\n",
    "            i = inc_size\n",
    "            continue\n",
    "        review_doc_get = review_doc.get\n",
    "        # Make dictionary of features\n",
    "        features = {feat: val for feat, val\n",
    "                    in BSON.decode(review_doc_get('nlp_features')).items()\n",
    "                    if val and val != float(\"NaN\")}\n",
    "        features.update({feat: review_doc_get(feat)\n",
    "                         for feat in non_nlp_features_to_use\n",
    "                         if feat != 'achievement_progress'\n",
    "                            and review_doc_get(feat)\n",
    "                            and review_doc_get(feat) != float(\"NaN\")})\n",
    "        # Add in the rest of the features\n",
    "        if 'achievement_progress' in non_nlp_features_to_use:\n",
    "            features.update({feat: val for feat, val\n",
    "                             in review_doc_get('achievement_progress').items()\n",
    "                             if val and val != float(\"NaN\")})\n",
    "        data.append(features)\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get development data\n",
    "#dev_data = get_dev_data(dev_cursor,\n",
    "#                        500,\n",
    "#                        non_nlp_feature_types + [hours_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "#v_dev = DictVectorizer(sparse=True)\n",
    "#X_dev = v_dev.fit_transform(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a K-means clusterer and fit it with the development data\n",
    "#num_clusters = 10\n",
    "#km = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n",
    "#km.fit(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clusters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Counter(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#km.cluster_centers_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labels = km.labels_\n",
    "#cluster_centers = km.cluster_centers_\n",
    "#km_labels_unique = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=num_clusters).fit(X_dev.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#km_pca = KMeans(init=pca.components_, n_clusters=num_clusters, n_init=1)\n",
    "#km_pca.fit(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#km_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_data_iteration(training_data_cursor,\n",
    "                             inc_size,\n",
    "                             non_nlp_features_to_use,\n",
    "                             y_feature):\n",
    "    '''\n",
    "    Get a list of training data dictionaries to use in model training.\n",
    "\n",
    "    :param training_data_cursor: cursor for training documents\n",
    "    :type training_data_cursor: pymongo.cursor.Cursor object\n",
    "    :param inc_size: number of training documents to extract\n",
    "    :type inc_size: int\n",
    "    :param non_nlp_features_to_use: list of non-NLP features to add into\n",
    "                                    the feature dictionaries \n",
    "    :type non_nlp_features_to_use: list of str\n",
    "    :param y_feature: feature to use as the \"score\"\n",
    "    :type y_feature: str\n",
    "    :returns: list of dict\n",
    "    '''\n",
    "\n",
    "    if inc_size < 1:\n",
    "        raise ValueError('inc_size parameter should be positive integer')\n",
    "    if y_feature in non_nlp_features_to_use:\n",
    "        raise Exception('y_feature must be a feature that is not in the '\n",
    "                        'non_nlp_features_to_use list')\n",
    "    data = []\n",
    "    i = 0\n",
    "    while i < inc_size:\n",
    "        try:\n",
    "            review_doc = next(training_data_cursor)\n",
    "        except StopIteration:\n",
    "            i = inc_size\n",
    "            continue\n",
    "        review_doc_get = review_doc.get\n",
    "        # Make dictionary of features\n",
    "        features = {feat: val for feat, val\n",
    "                    in BSON.decode(review_doc_get('nlp_features')).items()\n",
    "                    if val and val != float(\"NaN\")}\n",
    "        features.update({feat: review_doc_get(feat)\n",
    "                         for feat in non_nlp_features_to_use\n",
    "                         if feat != 'achievement_progress'\n",
    "                            and review_doc_get(feat)\n",
    "                            and review_doc_get(feat) != float(\"NaN\")})\n",
    "        # Add in the rest of the features\n",
    "        if 'achievement_progress' in non_nlp_features_to_use:\n",
    "            features.update({feat: val for feat, val\n",
    "                             in review_doc_get('achievement_progress').items()\n",
    "                             if val and val != float(\"NaN\")})\n",
    "        data.append(dict(y=review_doc_get(y_feature),\n",
    "                         id=review_doc_get(_id),\n",
    "                         x=features))\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_data(test_data_cursor,\n",
    "                  non_nlp_features_to_use,\n",
    "                  y_feature):\n",
    "    '''\n",
    "    Get a list of test data dictionaries to use in model testing.\n",
    "\n",
    "    :param test_data_cursor: cursor for test documents\n",
    "    :type test_data_cursor: pymongo.cursor.Cursor object\n",
    "    :param non_nlp_features_to_use: list of non-NLP features to add into\n",
    "                                    the feature dictionaries \n",
    "    :type non_nlp_features_to_use: list of str\n",
    "    :param y_feature: feature to use as the \"score\"\n",
    "    :type y_feature: str\n",
    "    :returns: list of dict\n",
    "    '''\n",
    "\n",
    "    if y_feature in non_nlp_features_to_use:\n",
    "        raise Exception('y_feature must be a feature that is not in the '\n",
    "                        'non_nlp_features_to_use list')\n",
    "    data = []\n",
    "    for review_doc in test_data_cursor:\n",
    "        review_doc_get = review_doc.get\n",
    "        # Make dictionary of features\n",
    "        features = {feat: val for feat, val\n",
    "                    in BSON.decode(review_doc_get('nlp_features')).items()\n",
    "                    if val and val != float(\"NaN\")}\n",
    "        features.update({feat: review_doc_get(feat)\n",
    "                         for feat in non_nlp_features_to_use\n",
    "                         if feat != 'achievement_progress'\n",
    "                            and review_doc_get(feat)\n",
    "                            and review_doc_get(feat) != float(\"NaN\")})\n",
    "        # Add in the rest of the features\n",
    "        if 'achievement_progress' in non_nlp_features_to_use:\n",
    "            features.update({feat: val for feat, val\n",
    "                             in review_doc_get('achievement_progress').items()\n",
    "                             if val and val != float(\"NaN\")})\n",
    "        data.append(dict(y=review_doc_get(y_feature),\n",
    "                         id=review_doc_get(_id),\n",
    "                         x=features))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats(_y_test, _y_preds):\n",
    "    \"\"\"\n",
    "    Get some statistics about the model's performance.\n",
    "\n",
    "    :param _y_test: test set values\n",
    "    :type _y_test: np.array\n",
    "    :param _y_train: predictions\n",
    "    :type _y_train: np.array\n",
    "    :returns: tuple\n",
    "    \"\"\"\n",
    "\n",
    "    _pearsonr = pearsonr(y_test,\n",
    "                           y_train_preds_1)\n",
    "    _r2 = r2_score(_y_test,\n",
    "                   _y_preds)\n",
    "    _prec_micro = precision_score(_y_test,\n",
    "                                  _y_preds,\n",
    "                                  labels=classes,\n",
    "                                  average='micro')\n",
    "    _prec_macro = precision_score(_y_test,\n",
    "                                  _y_preds,\n",
    "                                  labels=classes,\n",
    "                                  average='macro')\n",
    "    _prec_weighted = precision_score(_y_test,\n",
    "                                     _y_preds,\n",
    "                                     labels=classes,\n",
    "                                     average='weighted')\n",
    "    _f1_micro = f1_score(_y_test,\n",
    "                         _y_preds,\n",
    "                         labels=classes,\n",
    "                         average='micro')\n",
    "    _f1_macro = f1_score(_y_test,\n",
    "                         _y_preds,\n",
    "                         labels=classes,\n",
    "                         average='macro')\n",
    "    _f1_weighted = f1_score(_y_test,\n",
    "                            _y_preds,\n",
    "                            labels=classes,\n",
    "                            average='weighted')\n",
    "    _acc = accuracy_score(_y_test,\n",
    "                          _y_preds,\n",
    "                          normalize=True)\n",
    "    _conf_mat = confusion_matrix(_y_test,\n",
    "                                 _y_preds,\n",
    "                                 labels=classes)\n",
    "    \n",
    "    return (_pearsonr,\n",
    "            _r2,\n",
    "            _prec_micro,\n",
    "            _prec_macro,\n",
    "            _prec_weighted,\n",
    "            _f1_micro,\n",
    "            _f1_macro,\n",
    "            _f1_weighted,\n",
    "            _acc,\n",
    "            _conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = get_test_data(test_cursor,\n",
    "                          non_nlp_feature_types,\n",
    "                          hours_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 560394d4cbb14611d095887d\n",
      "total hours played value (binned): 1\n",
      "sample of features for review/reviewer: {\"'s storyline\": 1, 'like 270': 1, 'in rping': 1, '40 ft': 1, 'cause theres': 1, 'may mean': 1, 'expect strong': 1, 'with extreme': 1, 'decade': 1, 'triple-a': 1, 'brilliant': 1, '>killed': 1, 'asks why': 1, 'best bet': 1, 'ended whole': 1, '4000mhz': 1, 'scenario \"': 1, 'out this': 1, 'no-scope again': 1, 'm': 1, 'play wasteland': 1, 'continue to': 1, 'hill through': 1, 'mysteries': 1, 'inducing and': 1, \"n't really\": 1, 'and add': 1, 'rediculous': 1, 'the supercomputer': 1, '.. with': 1, '! (': 1, 'hard': 1, 'not on': 1, 'and e': 1, 'out rather': 1, \"cut '\": 1, 'competent with': 1, 'have mod': 1, 'evolve will': 1, 'rewards are': 1, 'from other': 1, 'bugs da': 1, 'about who': 1, 'item system': 1, 'real names': 1, 'downside ,': 1, 'like rpg': 1, 'unique weapons/vehicles': 1, 'lagy i': 1, \"thought i'd\": 1, 'simulator experience': 1, 'there if': 1, 'grenades ,': 1, 'milsimulator': 1, 'the menus': 1, 'best editor': 1, 'spawn but': 1, 'me this': 1, 'armor must': 1, 'are much': 1, 'are stunning': 1, 'put in)=': 1, 'helicopter pack': 1, 'a joy': 1, 'the lightning': 1, 'start to': 1, ', assigned': 1, 'mapping': 1, 'same elements': 1, 'with bohemia': 1, 'and wasted': 1, 'admit -': 1, 'of june': 1, 'overall can': 1, 'combinations': 1, 'amazing computer': 1, 'is little': 1, 'goats,deer and': 1, 'broom': 1, ' run': 1, 'better hit': 1, 'been shifted': 1, 'built it': 1, '>return to': 1, 'koth online': 1, 'failed': 1, 'except graphics': 1, 'from ,': 1, 'fixed before': 1, 'the stairs': 1, \"money i've\": 1, 'but delivered': 1, 'packed first': 1, 'here': 1, 'stratis island': 1, 'only horribly-done': 1, 'great and': 1, 'exploding i': 1, 'dayz related': 1, 'like hours': 1}\n"
     ]
    }
   ],
   "source": [
    "# Sample test data point\n",
    "print('id: ' + test_data[0]['id'])\n",
    "print('total hours played value (binned): ' + str(test_data[0]['y']))\n",
    "print('sample of features for review/reviewer: '\n",
    "      + str(dict(list(test_data[0]['x'].items())[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['560394d4cbb14611d095887d', '560394d4cbb14611d095887e',\n",
       "       '560394d4cbb14611d095887f', '560394d4cbb14611d0958880',\n",
       "       '560394d4cbb14611d0958881', '560394d4cbb14611d0958882',\n",
       "       '560394d4cbb14611d0958883', '560394d4cbb14611d0958884',\n",
       "       '560394d4cbb14611d0958885', '560394d4cbb14611d0958886'], \n",
       "      dtype='<U24')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids = np.array([_data['id'] for _data in test_data])\n",
    "test_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 1, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.array([_data['y'] for _data in test_data])\n",
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'! (': 1,\n",
       " \"'s storyline\": 1,\n",
       " '40 ft': 1,\n",
       " '4000mhz': 1,\n",
       " '>killed': 1,\n",
       " 'about who': 1,\n",
       " 'and add': 1,\n",
       " 'and e': 1,\n",
       " 'are stunning': 1,\n",
       " 'asks why': 1,\n",
       " 'best bet': 1,\n",
       " 'brilliant': 1,\n",
       " 'bugs da': 1,\n",
       " 'but delivered': 1,\n",
       " 'cause theres': 1,\n",
       " 'competent with': 1,\n",
       " 'continue to': 1,\n",
       " \"cut '\": 1,\n",
       " 'decade': 1,\n",
       " 'ended whole': 1,\n",
       " 'evolve will': 1,\n",
       " 'expect strong': 1,\n",
       " 'exploding i': 1,\n",
       " 'from other': 1,\n",
       " 'grenades ,': 1,\n",
       " 'hard': 1,\n",
       " 'have mod': 1,\n",
       " 'hill through': 1,\n",
       " 'in rping': 1,\n",
       " 'inducing and': 1,\n",
       " 'm': 1,\n",
       " 'may mean': 1,\n",
       " 'mysteries': 1,\n",
       " \"n't really\": 1,\n",
       " 'no-scope again': 1,\n",
       " 'not on': 1,\n",
       " 'only horribly-done': 1,\n",
       " 'out rather': 1,\n",
       " 'out this': 1,\n",
       " 'packed first': 1,\n",
       " 'play wasteland': 1,\n",
       " 'real names': 1,\n",
       " 'rediculous': 1,\n",
       " 'rewards are': 1,\n",
       " 'scenario \"': 1,\n",
       " 'stratis island': 1,\n",
       " 'the supercomputer': 1,\n",
       " \"thought i'd\": 1,\n",
       " 'triple-a': 1,\n",
       " 'with extreme': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature_dicts = [_data['x'] for _data in test_data]\n",
    "dict(list(test_feature_dicts[0].items())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Round of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Features\n",
    "- Vectorize the test set features\n",
    "- Vectorize a small portion of the training features (the first 100) and all of the test features, partially train the model, and then repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_1 = get_train_data_iteration(train_cursor,\n",
    "                                        100,\n",
    "                                        non_nlp_feature_types,\n",
    "                                        hours_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 560394d3cbb14611d0957f1c\n",
      "total hours played value (binned): 2\n",
      "sample of features for review/reviewer: {'odd': 1, 'cluster19': 1, 'play': 1, ' grea': 1, 'reat': 1, 'Acti': 1, '7 ': 1, 'om': 1, 'ial (': 1, 'ver': 1, ' Gr': 1, '10:AMOD:sound': 1, 'ga': 1, 'al th': 1, 'num_groups': 7, 'n ': 1, 'Actin': 1, 'eplay': 1, ' Ov': 1, 'g ': 1, '- voice': 1, 'miss': 1, 'at': 1, 'ra': 1, 'ics': 1, ' a': 1, 'ice A': 1, 'nd 1': 1, 'oi': 1, 'cluster63': 1, 'd ': 1, 'ddin': 1, 'os)': 1, '- Sou': 1, 'on s': 1, 'oddin': 1, 'ios': 1, 'g 1': 1, 'num_voted_helpfulness': 4, '10 -': 1, '5 - ': 1, 'a ': 1, 'potential': 1, 'ti': 1, ' (Mod': 1, 'sound 10': 1, 'me': 1, ' mis': 1, 'und': 1, 'num_comments': 1, ' a gr': 1, ' t': 1, 'ssi': 1, 'nario': 1, 'entia': 1, 'om m': 1, 'Moddi': 1, 'Game': 1, 'acting 10': 1, 'reat ': 1, 'Mod': 1, 'ting': 1, ' thi': 1, 'scenario:VMOD:custom': 1, 'ame': 1, 'y ': 1, '5 ': 1, 'th': 1, 'nd': 1, 'custo': 1, 'cen': 1, 'Voi': 1, 'e Ac': 1, 'al (M': 1, 'custom mission': 1, 'rap': 1, 'cluster7801': 1, 'lay': 1, 'er': 1, '- Vo': 1, 'ph': 1, 's i': 1, 'ddi': 1, 'scenario:VMOD:(': 1, 'ound ': 1, 'arios': 1, 'm mi': 1, 'cus': 1, 'this is': 1, 'a gr': 1, 'cluster1831': 1, 'scenario:VMOD:)': 1, 'a gre': 1, 'al ': 1, 'cluster6634': 1, 'oun': 1, 'be:ROOT:game': 1, 'pl': 1, 'num_reviews': 1, 'ics 9': 1}\n"
     ]
    }
   ],
   "source": [
    "# Sample training data point\n",
    "print('id: ' + train_data_1[0]['id'])\n",
    "print('total hours played value (binned): ' + str(train_data_1[0]['y']))\n",
    "print('sample of features for review/reviewer: '\n",
    "      + str(dict(list(train_data_1[0]['x'].items())[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids_1 = np.array([_data['id'] for _data in train_data_1])\n",
    "y_train_1 = np.array([_data['y'] for _data in train_data_1])\n",
    "train_feature_dicts_1 = [_data['x'] for _data in train_data_1]\n",
    "classes = np.unique(y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = DictVectorizer(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_1 = v.fit_transform(train_feature_dicts_1)\n",
    "X_test = v.transform(test_feature_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Perceptron learner\n",
    "learner = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'class_weight': None,\n",
       " 'eta0': 1.0,\n",
       " 'fit_intercept': True,\n",
       " 'n_iter': 5,\n",
       " 'n_jobs': 1,\n",
       " 'penalty': None,\n",
       " 'random_state': 0,\n",
       " 'shuffle': True,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.partial_fit(X_train_1,\n",
    "                    y_train_1,\n",
    "                    classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 3),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_preds_1 = learner.predict(X_test)\n",
    "[(y, y_pred) for y, y_pred in zip(y_test,\n",
    "                                  y_train_preds_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(pearsonr_1,\n",
    " r2_1,\n",
    " prec_micro_1,\n",
    " prec_macro_1,\n",
    " prec_weighted_1,\n",
    " f1_micro_1,\n",
    " f1_macro_1,\n",
    " f1_weighted_1,\n",
    " acc_1,\n",
    " conf_mat_1) = get_stats(y_test,\n",
    "                         y_train_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stats\n",
      "\n",
      "\n",
      "Pearson: 0.010476108371144748 (p = 0.8152388425640384)\n",
      "r2 score: -0.6156452332437088\n",
      "Micro precision score: 0.686\n",
      "Macro precision score: 0.30602240896358546\n",
      "Weighted precision score: 0.5489663865546218\n",
      "Micro f1 score: 0.686\n",
      "Macro f1 score: 0.29319793047827014\n",
      "Weighted f1 score: 0.5917310324616611\n",
      "Accuracy score: 0.686\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[341   6  11]\n",
      " [109   1   5]\n",
      " [ 26   0   1]]\n"
     ]
    }
   ],
   "source": [
    "print('Some stats\\n\\n')\n",
    "print('Pearson: {} (p = {})'.format(*pearsonr_1))\n",
    "print('r2 score: {}'.format(r2_1))\n",
    "print('Micro precision score: {}'.format(prec_micro_1))\n",
    "print('Macro precision score: {}'.format(prec_macro_1))\n",
    "print('Weighted precision score: {}'.format(prec_weighted_1))\n",
    "print('Micro f1 score: {}'.format(f1_micro_1))\n",
    "print('Macro f1 score: {}'.format(f1_macro_1))\n",
    "print('Weighted f1 score: {}'.format(f1_weighted_1))\n",
    "print('Accuracy score: {}'.format(acc_1))\n",
    "print('\\nConfusion matrix:\\n\\n{}'.format(conf_mat_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Round of Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 560394d3cbb14611d0957f80\n",
      "total hours played value (binned): 1\n",
      "sample of features for review/reviewer: {'wrong game,this': 1, 'in rping': 1, 'your kdr': 1, 'therefore': 1, 'game of': 1, 'campaign are': 1, 'bugs': 1, 'so i': 1, 'only as': 1, 'real stress': 1, 'options of': 1, 'better': 1, 'for work': 1, 'job': 1, 'scenario \"': 1, 'if he': 1, 'there are': 1, 'mods are': 1, 'on the': 1, 'rt': 1, 'hill through': 1, 'by 4': 1, ' act': 1, 'telling': 1, 'and glitches': 1, 'them ,': 1, ', especailly': 1, 'hard': 1, 'open-ended goals': 1, 'and e': 1, 'for them': 1, 'cluster63': 1, 'will continue': 1, 'an objective': 1, 's, ': 1, 'playing this': 1, 'requires': 1, 'mods providing': 1, 'ich ': 1, '10-25fps': 1, 'actual': 1, 'great and': 1, 'endless amounts': 1, 'support': 1, 'accessible': 1, 'combat .': 1, 'guns': 1, 'chatting': 1, 'me this': 1, 'definetly': 1, 'moves': 1, 'inves': 1, 'mean huge.extreme': 1, 'sounds like': 1, ', go': 1, 'physics': 1, 'experience .': 1, 'finally knew': 1, 'f m': 1, 'etc.though as': 1, ' :': 1, 'got this': 1, ' loa': 1, 'gun given': 1, 'anyway .': 1, 'saw about': 1, 'share,or download': 1, 'o it': 1, 'a double': 1, 'kdr': 1, 'crash into': 1, 'overall the': 1, 'lost in': 1, ' gam': 1, 'so ;': 1, 'in dlc': 1, 'the military': 1, 'failed': 1, 'get used': 1, 'do:SBAR:but': 1, 'in chat': 1, 'ground and': 1, 'updates to': 1, 'and self-accomplishment': 1, 'reaslitic': 1, 'mmend': 1, 'rds b': 1, 'against other': 1, 'game wo': 1, 'healthy': 1, 'less fun': 1, 'explosions': 1, 'writing a': 1, 'maybe': 1, 'bit:OBJ:of': 1, 'optimised game': 1, 'vs my': 1, 'job .': 1, 'game will': 1, ' pro': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data_2 = get_train_data_iteration(train_cursor,\n",
    "                                        100,\n",
    "                                        non_nlp_feature_types,\n",
    "                                        hours_feature)\n",
    "# Sample training data point\n",
    "print('id: ' + train_data_2[0]['id'])\n",
    "print('total hours played value (binned): ' + str(train_data_2[0]['y']))\n",
    "print('sample of features for review/reviewer: '\n",
    "      + str(dict(list(train_data_2[0]['x'].items())[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids_2 = np.array([_data['id'] for _data in train_data_2])\n",
    "y_train_2 = np.array([_data['y'] for _data in train_data_2])\n",
    "train_feature_dicts_2 = [_data['x'] for _data in train_data_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = v.transform(train_feature_dicts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = v.transform(test_feature_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.partial_fit(X_train_2,\n",
    "                    y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 3),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (3, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_preds_2 = learner.predict(X_test)\n",
    "[(y, y_pred) for y, y_pred in zip(y_test,\n",
    "                                  y_train_preds_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stats\n",
      "\n",
      "\n",
      "Pearson: 0.010476108371144748 (p = 0.8152388425640384)\n",
      "r2 score: -0.9532427446677674\n",
      "Micro precision score: 0.604\n",
      "Macro precision score: 0.3745277917622271\n",
      "Weighted precision score: 0.6159959277283661\n",
      "Micro f1 score: 0.604\n",
      "Macro f1 score: 0.3722430295464003\n",
      "Weighted f1 score: 0.6094113607990013\n",
      "Accuracy score: 0.604\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[269  62  27]\n",
      " [ 70  30  15]\n",
      " [ 15   9   3]]\n"
     ]
    }
   ],
   "source": [
    "(pearsonr_2,\n",
    " r2_2,\n",
    " prec_micro_2,\n",
    " prec_macro_2,\n",
    " prec_weighted_2,\n",
    " f1_micro_2,\n",
    " f1_macro_2,\n",
    " f1_weighted_2,\n",
    " acc_2,\n",
    " conf_mat_2) = get_stats(y_test,\n",
    "                         y_train_preds_2)\n",
    "print('Some stats\\n\\n')\n",
    "print('Pearson: {} (p = {})'.format(*pearsonr_2))\n",
    "print('r2 score: {}'.format(r2_2))\n",
    "print('Micro precision score: {}'.format(prec_micro_2))\n",
    "print('Macro precision score: {}'.format(prec_macro_2))\n",
    "print('Weighted precision score: {}'.format(prec_weighted_2))\n",
    "print('Micro f1 score: {}'.format(f1_micro_2))\n",
    "print('Macro f1 score: {}'.format(f1_macro_2))\n",
    "print('Weighted f1 score: {}'.format(f1_weighted_2))\n",
    "print('Accuracy score: {}'.format(acc_2))\n",
    "print('\\nConfusion matrix:\\n\\n{}'.format(conf_mat_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Round of Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 560394d3cbb14611d0957fe4\n",
      "total hours played value (binned): 1\n",
      "sample of features for review/reviewer: {'in rping': 1, 'game of': 1, 'bugs': 1, 'lakeside': 1, 'only as': 1, 'options of': 1, '. allowing': 1, 'easy to': 1, 'everyday !': 1, 'scenario \"': 1, 'out this': 1, 'realistic but': 1, 'houses/outhouses/farms/industrial': 1, 'mods are': 1, 'd dog': 1, 'rt': 1, 'hill through': 1, \"n't really\": 1, ', especailly': 1, 'die ,': 1, 'hard': 1, 'open-ended goals': 1, 'very:ROOT:and': 1, 'for them': 1, 'cluster63': 1, 'spots .': 1, 'an objective': 1, 'for people': 1, 'requires': 1, 'mods providing': 1, 'best pcs': 1, 'item system': 1, 'great and': 1, 'endless amounts': 1, 'military simulators': 1, 'partner then': 1, 'combat .': 1, 'call air': 1, 'chatting': 1, 'moves': 1, 'bleeding to': 1, 'me this': 1, 'at medium': 1, 'game truly': 1, 'physics': 1, 'location you': 1, 'grandmother and': 1, 'and group': 1, 'gun given': 1, 'saw about': 1, 'owned': 1, ' goo': 1, \"game 's\": 1, 'r9 280x': 1, 'crash into': 1, 'this must': 1, 'so ;': 1, 'in dlc': 1, 'chatter': 1, 'can work': 1, 'failed': 1, 'island ,': 1, 'get used': 1, 'is creating': 1, 'ground and': 1, \"and i've\": 1, 'whole thing': 1, 'must buy': 1, 'finishing': 1, 'healthy': 1, 'played dayz': 1, 'disturbed': 1, 'pay attention': 1, 'jason .': 1, 'editor :)': 1, '8 :': 1, 'when the': 1, 'give it': 1, 'ly r': 1, 'range ,': 1, 'flashpoint': 1, 'a situation': 1, 'auto setting': 1, 'then i': 1, 'grass greens': 1, 'and heart': 1, 'at lz': 1, 'such as': 1, 'who craves': 1, 'banned': 1, 'year': 1, 'pilfer': 1, '% finished': 1, 'edition of': 1, 'login': 1, 'ommen': 1, 'you this': 1, 'figuring out': 1, \"smokin '\": 1, 'end ': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data_3 = get_train_data_iteration(train_cursor,\n",
    "                                        100,\n",
    "                                        non_nlp_feature_types,\n",
    "                                        hours_feature)\n",
    "print('id: ' + train_data_3[0]['id'])\n",
    "print('total hours played value (binned): ' + str(train_data_3[0]['y']))\n",
    "print('sample of features for review/reviewer: '\n",
    "      + str(dict(list(train_data_3[0]['x'].items())[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids_3 = np.array([_data['id'] for _data in train_data_3])\n",
    "y_train_3 = np.array([_data['y'] for _data in train_data_3])\n",
    "train_feature_dicts_3 = [_data['x'] for _data in train_data_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_3 = v.transform(train_feature_dicts_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = v.transform(test_feature_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.partial_fit(X_train_3,\n",
    "                    y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (2, 3),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (3, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (3, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (3, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 3),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (3, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (3, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (3, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (3, 3),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (3, 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_preds_3 = learner.predict(X_test)\n",
    "[(y, y_pred) for y, y_pred in zip(y_test,\n",
    "                                  y_train_preds_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stats\n",
      "\n",
      "\n",
      "Pearson: 0.010476108371144748 (p = 0.8152388425640384)\n",
      "r2 score: -2.1348340346519725\n",
      "Micro precision score: 0.35\n",
      "Macro precision score: 0.30391928946113494\n",
      "Weighted precision score: 0.5169084763948498\n",
      "Micro f1 score: 0.35\n",
      "Macro f1 score: 0.2696981990288278\n",
      "Weighted f1 score: 0.39151321900547054\n",
      "Accuracy score: 0.35\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[126 183  49]\n",
      " [ 50  44  21]\n",
      " [ 16   6   5]]\n"
     ]
    }
   ],
   "source": [
    "(pearsonr_3,\n",
    " r2_3,\n",
    " prec_micro_3,\n",
    " prec_macro_3,\n",
    " prec_weighted_3,\n",
    " f1_micro_3,\n",
    " f1_macro_3,\n",
    " f1_weighted_3,\n",
    " acc_3,\n",
    " conf_mat_3) = get_stats(y_test,\n",
    "                         y_train_preds_3)\n",
    "print('Some stats\\n\\n')\n",
    "print('Pearson: {} (p = {})'.format(*pearsonr_3))\n",
    "print('r2 score: {}'.format(r2_3))\n",
    "print('Micro precision score: {}'.format(prec_micro_3))\n",
    "print('Macro precision score: {}'.format(prec_macro_3))\n",
    "print('Weighted precision score: {}'.format(prec_weighted_3))\n",
    "print('Micro f1 score: {}'.format(f1_micro_3))\n",
    "print('Macro f1 score: {}'.format(f1_macro_3))\n",
    "print('Weighted f1 score: {}'.format(f1_weighted_3))\n",
    "print('Accuracy score: {}'.format(acc_3))\n",
    "print('\\nConfusion matrix:\\n\\n{}'.format(conf_mat_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Round of Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 560394d3cbb14611d0958048\n",
      "total hours played value (binned): 1\n",
      "sample of features for review/reviewer: {'in rping': 1, 'my take': 1, 'game of': 1, 'doubt ,': 1, 'bugs': 1, 'lakeside': 1, 'only as': 1, 'options of': 1, ', restart': 1, '. allowing': 1, '>killed': 1, 'asks why': 1, 'koth online': 1, 'glorious but': 1, 'scenario \"': 1, 'out this': 1, 'realistic but': 1, 'houses/outhouses/farms/industrial': 1, 'mods are': 1, 'sandbox open': 1, 'fps since': 1, 'continue to': 1, 'hill through': 1, \"n't really\": 1, 'played dayz': 1, ', especailly': 1, 'die ,': 1, 'hard': 1, 'not on': 1, 'and e': 1, 'for them': 1, 'that refers': 1, 'competent with': 1, \"n't see\": 1, 'dem': 1, 'an objective': 1, 'for people': 1, 'great game': 1, 'requires': 1, 'mods providing': 1, 'ai in': 1, 'day and': 1, 'best pcs': 1, 'item system': 1, 'great and': 1, 'endless amounts': 1, 'military simulators': 1, 'partner then': 1, 'combat .': 1, 'call air': 1, 'chatting': 1, 'releasing': 1, 'moves': 1, 'bleeding to': 1, 'me this': 1, 'at medium': 1, 'game truly': 1, 'physics': 1, 'location you': 1, 'grandmother and': 1, 'helicopter pack': 1, 'and group': 1, 'firefights': 1, 'gun given': 1, 'saw about': 1, 'shure': 1, 'making missions': 1, 'a double': 1, \"game 's\": 1, 'is little': 1, 'r9 280x': 1, 'annoyance': 1, 'this must': 1, 'so ;': 1, 'in dlc': 1, '>return to': 1, 'can work': 1, 'failed': 1, 'everyday !': 1, 'get used': 1, 'is creating': 1, 'ground and': 1, \"and i've\": 1, 'whole thing': 1, 'must buy': 1, 'east of': 1, 'men step': 1, 'healthy': 1, 'tldr': 1, 'disturbed': 1, 'those legendary': 1, '3s kitchen': 1, 'maybe': 1, \"i've spent\": 1, '8 :': 1, 'victory glorious': 1, 'money ,': 1, 'flashpoint': 1, 'the arma3': 1, 'shaky': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data_4 = get_train_data_iteration(train_cursor,\n",
    "                                        100,\n",
    "                                        non_nlp_feature_types,\n",
    "                                        hours_feature)\n",
    "print('id: ' + train_data_4[0]['id'])\n",
    "print('total hours played value (binned): ' + str(train_data_4[0]['y']))\n",
    "print('sample of features for review/reviewer: '\n",
    "      + str(dict(list(train_data_4[0]['x'].items())[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids_4 = np.array([_data['id'] for _data in train_data_4])\n",
    "y_train_4 = np.array([_data['y'] for _data in train_data_4])\n",
    "train_feature_dicts_4 = [_data['x'] for _data in train_data_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_4 = v.transform(train_feature_dicts_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = v.transform(test_feature_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.partial_fit(X_train_4,\n",
    "                    y_train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 3),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (1, 1),\n",
       " (1, 3),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (3, 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_preds_4 = learner.predict(X_test)\n",
    "[(y, y_pred) for y, y_pred in zip(y_test,\n",
    "                                  y_train_preds_4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stats\n",
      "\n",
      "\n",
      "Pearson: 0.010476108371144748 (p = 0.8152388425640384)\n",
      "r2 score: -0.48301763946997145\n",
      "Micro precision score: 0.7\n",
      "Macro precision score: 0.3346100759144237\n",
      "Weighted precision score: 0.5663757763975156\n",
      "Micro f1 score: 0.7\n",
      "Macro f1 score: 0.2985116416150899\n",
      "Weighted f1 score: 0.5991542882404951\n",
      "Accuracy score: 0.7\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[348   3   7]\n",
      " [110   1   4]\n",
      " [ 25   1   1]]\n"
     ]
    }
   ],
   "source": [
    "(pearsonr_4,\n",
    " r2_4,\n",
    " prec_micro_4,\n",
    " prec_macro_4,\n",
    " prec_weighted_4,\n",
    " f1_micro_4,\n",
    " f1_macro_4,\n",
    " f1_weighted_4,\n",
    " acc_4,\n",
    " conf_mat_4) = get_stats(y_test,\n",
    "                         y_train_preds_4)\n",
    "print('Some stats\\n\\n')\n",
    "print('Pearson: {} (p = {})'.format(*pearsonr_4))\n",
    "print('r2 score: {}'.format(r2_4))\n",
    "print('Micro precision score: {}'.format(prec_micro_4))\n",
    "print('Macro precision score: {}'.format(prec_macro_4))\n",
    "print('Weighted precision score: {}'.format(prec_weighted_4))\n",
    "print('Micro f1 score: {}'.format(f1_micro_4))\n",
    "print('Macro f1 score: {}'.format(f1_macro_4))\n",
    "print('Weighted f1 score: {}'.format(f1_weighted_4))\n",
    "print('Accuracy score: {}'.format(acc_4))\n",
    "print('\\nConfusion matrix:\\n\\n{}'.format(conf_mat_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fifth Round of Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
